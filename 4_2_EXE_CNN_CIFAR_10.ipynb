{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZW0gaQO81Sq"
      },
      "source": [
        "# CNN on CIFAR-10\n",
        "\n",
        "In this notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.\n",
        "\n",
        "The images in CIFAR-10 are RGB images (3 channels) with size 32x32 (so they have size 3x32x32). There are 10 different classes. See examples below.\n",
        "\n",
        "![cifar10](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/cifar10.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htyg7xxN81St"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v3u2GIWr81Su"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used device:\n",
            "cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn import metrics\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Switch to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Used device:\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "def accuracy(target, pred):\n",
        "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "\n",
        "def compute_confusion_matrix(target, pred, normalize=None):\n",
        "    return metrics.confusion_matrix(\n",
        "        target.detach().cpu().numpy(), \n",
        "        pred.detach().cpu().numpy(),\n",
        "        normalize=normalize\n",
        "    )\n",
        "\n",
        "def show_image(img):\n",
        "    img = img.detach().cpu()\n",
        "    img = img / 2 + 0.5   # unnormalize\n",
        "    with sns.axes_style(\"white\"):\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img.permute((1, 2, 0)).numpy())\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "8f79e562a09d431ebd01e9b246a3f1d7",
            "32ce28063fa54dcbaa8af659704b70e0",
            "a5cf235ab55d498f85b81fe16acf5ee2",
            "06fde1e81a9f42c8b139c3af6a5e607d",
            "0eea03ce5fed4c8cb7a99fae91d71072",
            "ed02c289450e4525b5a8db5400462c6f",
            "96412304389842de8ecf5fce9dfddb25",
            "b4976902095e44249af4089e1dabf172",
            "d04d44a39f4a49ed9c0e74bb5f138453",
            "3ac5b071dd6d4fa2a6593767ff02860e",
            "a6f670e6201647a1bed7484072da4d78"
          ]
        },
        "id": "QZeTujLC81S3",
        "outputId": "429faa4b-2419-4cb0-cdb8-d60907583eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# The output of torchvision datasets are PIL images in the range [0, 1]. \n",
        "# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size = 64  # both for training and testing\n",
        "\n",
        "# Load datasets\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
        "\n",
        "# Map from class index to class name.\n",
        "classes = {index: name for name, index in train_set.class_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDHkc52L81S9",
        "outputId": "25a0628d-5016-479a-d610-f5cdc600bcc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data\n",
            "Number of points: 50000\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jonas\\OneDrive - Danmarks Tekniske Universitet\\02456 - Deep Learning\\4_2_EXE_CNN_CIFAR_10.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of points:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(train_set))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch dimension (B x C x H x W):\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of distinct labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(train_set\u001b[39m.\u001b[39mtargets))\u001b[39m}\u001b[39;00m\u001b[39m (unique labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mset\u001b[39m(train_set\u001b[39m.\u001b[39mtargets)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    163\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 164\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "print(\"Training data\")\n",
        "print(\"Number of points:\", len(train_set))\n",
        "x, y = next(iter(train_loader))\n",
        "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
        "print(f\"Number of distinct labels: {len(set(train_set.targets))} (unique labels: {set(train_set.targets)})\")\n",
        "\n",
        "print(\"\\nTest data\")\n",
        "print(\"Number of points:\", len(test_set))\n",
        "x, y = next(iter(test_loader))\n",
        "print(\"Batch dimension (B x C x H x W):\", x.shape)\n",
        "print(f\"Number of distinct labels: {len(set(test_set.targets))} (unique labels: {set(test_set.targets)})\")\n",
        "\n",
        "n_classes = len(set(test_set.targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSA1h94681TB"
      },
      "source": [
        "### Show example images\n",
        "\n",
        "Run multiple times to see different examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "njJy0klP81TD",
        "outputId": "50fd3855-1a2a-4301-c3ee-005a7215d157"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jonas\\OneDrive - Danmarks Tekniske Universitet\\02456 - Deep Learning\\4_2_EXE_CNN_CIFAR_10.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get random training images and show them.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(train_loader)\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/02456%20-%20Deep%20Learning/4_2_EXE_CNN_CIFAR_10.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m show_image(torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmake_grid(images))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    163\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 164\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "# Get random training images and show them.\n",
        "images, labels = iter(train_loader).next()\n",
        "show_image(torchvision.utils.make_grid(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3BVFMF81TI"
      },
      "source": [
        "## Define a convolutional neural network\n",
        "\n",
        "\n",
        "**Assignment 1:** Define a convolutional neural network. \n",
        "You may use the code from previous notebooks.\n",
        "We suggest that you start with a small network, and make sure that everything is working.\n",
        "Once you can train successfully, come back and improve the architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYzzEhFB7StP"
      },
      "source": [
        "**Network Structure** \\\n",
        "- 3 channeled 32x32 input layer\n",
        "- 16 channel convolutional layer with 5x5 kernel, stride of 1 and 0 padding\n",
        "- 100 neuron dense layer\n",
        "- 10 class output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EsKbw3o81TK",
        "outputId": "659db298-7ed7-4a87-a1c1-f6a87e1578c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (dense_out): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout, Sequential, ReLU\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
        "\n",
        "# hyperameters of the model\n",
        "num_classes = 10\n",
        "channels = x.shape[1]\n",
        "height = x.shape[2]\n",
        "width = x.shape[3]\n",
        "num_filters_conv = [32, 64, 64]\n",
        "kernel_size_conv = [3, 3, 3]\n",
        "stride_conv = [1, 1, 1]\n",
        "padding_conv = [0, 0, 0]\n",
        "\n",
        "num_l1 = 64\n",
        "\n",
        "class PrintSize(nn.Module):\n",
        "    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first pass.\"\"\"\n",
        "    \n",
        "    first = True\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.first:\n",
        "            print(f\"Size: {x.size()}\")\n",
        "            self.first = False\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "\n",
        "        #\n",
        "        conv_kernel_size = 3\n",
        "        pool_kernal_size = 2\n",
        "        stride = 1\n",
        "\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.layer1 = Sequential(\n",
        "            Conv2d(\n",
        "                in_channels = channels,\n",
        "                out_channels = num_filters_conv[0],\n",
        "                kernel_size = conv_kernel_size,\n",
        "                stride = stride\n",
        "            ),\n",
        "            BatchNorm2d(num_filters_conv[0]),\n",
        "            ReLU(),\n",
        "            MaxPool2d(pool_kernal_size)\n",
        "        )\n",
        "        self.layer2 = Sequential(\n",
        "            Conv2d(\n",
        "                in_channels = num_filters_conv[0],\n",
        "                out_channels = num_filters_conv[1],\n",
        "                kernel_size = conv_kernel_size,\n",
        "                stride = stride\n",
        "            ),\n",
        "            BatchNorm2d(num_filters_conv[1]),\n",
        "            ReLU(),\n",
        "            MaxPool2d(pool_kernal_size)\n",
        "        )\n",
        "        self.layer3 = Sequential(\n",
        "            Conv2d(\n",
        "                in_channels = num_filters_conv[1],\n",
        "                out_channels = num_filters_conv[2],\n",
        "                kernel_size = conv_kernel_size,\n",
        "                stride = stride\n",
        "            ),\n",
        "            BatchNorm2d(num_filters_conv[2]),\n",
        "            ReLU(),\n",
        "            MaxPool2d(pool_kernal_size)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.l1_in_features = 256\n",
        "\n",
        "        # Dense layers\n",
        "        self.dense_out = Sequential(\n",
        "            Linear(\n",
        "                in_features = self.l1_in_features,\n",
        "                out_features = num_l1,\n",
        "                bias = True\n",
        "            ),\n",
        "            Linear(\n",
        "                in_features = num_l1, \n",
        "                out_features = num_classes,\n",
        "                bias = False\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.layer1(x)\n",
        "      x = self.layer2(x)\n",
        "      x = self.layer3(x)\n",
        "      x = x.view(-1, self.l1_in_features)\n",
        "      x = self.dense_out(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "model = Model(n_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Switch to GPU if available\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-IUg3sq81TQ"
      },
      "source": [
        "## Define a loss function and optimizer\n",
        "\n",
        "**Assignment 2:** Define the loss function and optimizer.\n",
        "You might need to experiment a bit with the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "48AX85QP81TR"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WneIN7C81TV"
      },
      "source": [
        "## Train the network\n",
        "\n",
        "**Assignment 3:** Finish the training loop below. \n",
        "Start by using a small number of epochs (e.g. 2).\n",
        "Even with a low number of epochs you should be able to see results that are better than chance.\n",
        "When everything is working increase the number of epochs to find out how good your network really is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWT0ULctWvm1",
        "outputId": "b4face79-8097-4c5a-8725-6d95e5767477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([2, 10])\n",
            "Output logits:\n",
            "[[ 5.88183045e-01  1.30826578e-01  1.03082073e+00 -7.80658603e-01\n",
            "  -1.38925046e-01 -3.60266507e-01 -9.62196887e-02 -2.37300992e-01\n",
            "   1.01466291e-01 -6.99186862e-01]\n",
            " [ 8.03375125e-01  2.73386016e-04  8.83033037e-01 -3.66170794e-01\n",
            "  -4.23322290e-01 -1.08577535e-01  2.96730369e-01 -3.50223064e-01\n",
            "   4.36824620e-01 -6.90847993e-01]]\n",
            "Output probabilities:\n",
            "[[0.16265818 0.10295526 0.2532276  0.04138048 0.07861345 0.0630042\n",
            "  0.08204339 0.07124802 0.09997641 0.04489296]\n",
            " [0.18615325 0.08338501 0.20158844 0.05780206 0.0545912  0.07478502\n",
            "  0.1121599  0.05873125 0.12902676 0.04177705]]\n"
          ]
        }
      ],
      "source": [
        "# Test the forward pass with dummy data\n",
        "out = model(torch.randn(2, 3, 32, 32, device=device))\n",
        "print(\"Output shape:\", out.size())\n",
        "print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
        "print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NkUanRRb81TW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500     training accuracy: 0.531\n",
            "             test accuracy: 0.5862\n",
            "Step 1000    training accuracy: 0.664420871559633\n",
            "             test accuracy: 0.6285\n",
            "Step 1500    training accuracy: 0.685125\n",
            "             test accuracy: 0.6622\n",
            "Step 2000    training accuracy: 0.7223337155963303\n",
            "             test accuracy: 0.7102\n",
            "Step 2500    training accuracy: 0.7544642857142857\n",
            "             test accuracy: 0.7149\n",
            "Step 3000    training accuracy: 0.75421875\n",
            "             test accuracy: 0.7082\n",
            "Step 3500    training accuracy: 0.7762936827956989\n",
            "             test accuracy: 0.7246\n",
            "Step 4000    training accuracy: 0.7994791666666666\n",
            "             test accuracy: 0.739\n",
            "Step 4500    training accuracy: 0.79265625\n",
            "             test accuracy: 0.7417\n",
            "Step 5000    training accuracy: 0.8125\n",
            "             test accuracy: 0.7413\n",
            "Step 5500    training accuracy: 0.8347355769230769\n",
            "             test accuracy: 0.7427\n",
            "Step 6000    training accuracy: 0.8191875\n",
            "             test accuracy: 0.74\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "validation_every_steps = 500\n",
        "\n",
        "step = 0\n",
        "model.train()\n",
        "\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "        \n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_accuracies_batches = []\n",
        "    \n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Forward pass, compute gradients, perform one training step.\n",
        "        output = model(inputs)\n",
        "        batch_loss = loss_fn(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increment step counter\n",
        "        step += 1\n",
        "        \n",
        "        # Compute accuracy.\n",
        "        predictions = output.max(1)[1]\n",
        "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "        \n",
        "        if step % validation_every_steps == 0:\n",
        "            \n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "            \n",
        "            train_accuracies_batches = []\n",
        "        \n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches = []\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for inputs, targets in test_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = model(inputs)\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = output.max(1)[1]\n",
        "\n",
        "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "                model.train()\n",
        "                \n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
        "     \n",
        "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qAsbC8I81Ta"
      },
      "source": [
        "## Test the network\n",
        "\n",
        "Now we show a batch of test images and generate a table below with the true and predicted class for each of these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LT0RoAC81Tc"
      },
      "outputs": [],
      "source": [
        "inputs, targets = iter(test_loader).next()\n",
        "inputs, targets = inputs.to(device), targets.to(device)\n",
        "show_image(make_grid(inputs))\n",
        "plt.show()\n",
        "\n",
        "outputs = model(inputs)\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "print(\"    TRUE        PREDICTED\")\n",
        "print(\"-----------------------------\")\n",
        "for target, pred in zip(targets, predicted):\n",
        "    print(f\"{classes[target.item()]:^13} {classes[pred.item()]:^13}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISA6LJJO81Tg"
      },
      "source": [
        "We now evaluate the network as above, but on the entire test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smv6_BwF81Ti"
      },
      "outputs": [],
      "source": [
        "# Evaluate test set\n",
        "confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_accuracies = []\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        predictions = output.max(1)[1]\n",
        "\n",
        "        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n",
        "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
        "        \n",
        "        confusion_matrix += compute_confusion_matrix(targets, predictions)\n",
        "\n",
        "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
        "    \n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylxBAyXwI7Ab"
      },
      "source": [
        "Here we report the **average test accuracy** (number of correct predictions divided by test set size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5-c_CDAI7Ab"
      },
      "outputs": [],
      "source": [
        "print(f\"Test accuracy: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfUqY9rI7Ab"
      },
      "source": [
        "Here we look a bit more in depth into the performance of the classifier, using the **confusion matrix**. The entry at the i-th row and j-th column indicates the number of samples with true label being the i-th class and predicted label being the j-th class.\n",
        "\n",
        "We normalize the rows: given all examples of a specific class (row), we can observe here how they are classified by our model. Ideally, we would like the entries on the diagonals to be 1, and everything else 0. This would mean that all examples from that class are classified correctly.\n",
        "\n",
        "The classes that are harder to classify for our model have lower numbers on the diagonal. We can then see exactly *how* they are misclassified by looking at the rest of the row.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShcEZDExI7Ab"
      },
      "outputs": [],
      "source": [
        "def normalize(matrix, axis):\n",
        "    axis = {'true': 1, 'pred': 0}[axis]\n",
        "    return matrix / matrix.sum(axis=axis, keepdims=True)\n",
        "\n",
        "x_labels = [classes[i] for i in classes]\n",
        "y_labels = x_labels\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(\n",
        "    ax=plt.gca(),\n",
        "    data=normalize(confusion_matrix, 'true'),\n",
        "    annot=True,\n",
        "    linewidths=0.5,\n",
        "    cmap=\"Reds\",\n",
        "    cbar=False,\n",
        "    fmt=\".2f\",\n",
        "    xticklabels=x_labels,\n",
        "    yticklabels=y_labels,\n",
        ")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.ylabel(\"True class\")\n",
        "plt.xlabel(\"Predicted class\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw13Y86VI7Ac"
      },
      "source": [
        "Here we focus on the diagonal and plot the numbers in a bar plot. This gives us a clearer picture of the accuracy of the model for different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv75wMhVI7Ac"
      },
      "outputs": [],
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x=x_labels, y=np.diag(normalize(confusion_matrix, 'true')))\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(\"Per-class accuracy\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocnQOBAl81Tn"
      },
      "source": [
        "**Assignment 4:** \n",
        "1. Go back and improve performance of the network. By using enough convolutional layers with enough channels (and by training for long enough), you should easily be able to get a test accuracy above 60%, but see how much further you can get it! Can you reach 70%?\n",
        "\n",
        "2. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
        "Did anything surprise you during the exercise?\n",
        "What were the changes that seemed to improve performance the most?\n",
        "\n",
        "3. Write down key lessons/insights you got during this exercise.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nzefavy81To"
      },
      "source": [
        "# Training on GPU\n",
        "\n",
        "**Optional Assignment:**\n",
        "If you have a GPU, we suggest that you try training your model on GPU. For this, you need to move the model to GPU after defining it, which will recursively go over all modules and convert their parameters and buffers to CUDA tensors. You also need to transfer both the inputs and targets to GPU at each training step, before performing the forward pass.\n",
        "\n",
        "The code for this is already in place: notice the `.to(device)` statements. The only thing left to do is change the definition of `device` from `'cpu'` to `'cuda'`.\n",
        "\n",
        "If you don't have a GPU, you can do this on [Google Colab](https://research.google.com/colaboratory/).\n",
        "\n",
        "Use the code below to check if any GPU is avaiable in your current setup. This should print the models of all available GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I09rtehbaCRH"
      },
      "outputs": [],
      "source": [
        "# Check if we have GPUs available\n",
        "print(\"Available CUDA devices:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actFGqQZM9Vd"
      },
      "source": [
        "You may not notice any significant speed-up from using a GPU. This is probably because your network is really small. Try increasing the width of your network (number of channels in the convolutional layers) and see if you observe any speed-up on GPU compared to CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8mEIylU81Tp"
      },
      "source": [
        "# Exercise from Michael Nielsen's book\n",
        "\n",
        "**Assignment 5:** Pick an exercise of your own choice from [Michael Nielsen's book](http://neuralnetworksanddeeplearning.com/).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh7KTAdWD_zx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "059b92ebffe316512df3810dcdd9739bd5d694b60baa1e9e8136193b1cf34557"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06fde1e81a9f42c8b139c3af6a5e607d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ac5b071dd6d4fa2a6593767ff02860e",
            "placeholder": "​",
            "style": "IPY_MODEL_a6f670e6201647a1bed7484072da4d78",
            "value": " 170498071/170498071 [00:02&lt;00:00, 71478718.25it/s]"
          }
        },
        "0eea03ce5fed4c8cb7a99fae91d71072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32ce28063fa54dcbaa8af659704b70e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed02c289450e4525b5a8db5400462c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_96412304389842de8ecf5fce9dfddb25",
            "value": "100%"
          }
        },
        "3ac5b071dd6d4fa2a6593767ff02860e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f79e562a09d431ebd01e9b246a3f1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32ce28063fa54dcbaa8af659704b70e0",
              "IPY_MODEL_a5cf235ab55d498f85b81fe16acf5ee2",
              "IPY_MODEL_06fde1e81a9f42c8b139c3af6a5e607d"
            ],
            "layout": "IPY_MODEL_0eea03ce5fed4c8cb7a99fae91d71072"
          }
        },
        "96412304389842de8ecf5fce9dfddb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5cf235ab55d498f85b81fe16acf5ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4976902095e44249af4089e1dabf172",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d04d44a39f4a49ed9c0e74bb5f138453",
            "value": 170498071
          }
        },
        "a6f670e6201647a1bed7484072da4d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4976902095e44249af4089e1dabf172": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04d44a39f4a49ed9c0e74bb5f138453": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed02c289450e4525b5a8db5400462c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
